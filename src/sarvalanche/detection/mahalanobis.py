import numpy as np
import xarray as xr
import torch
from pathlib import Path
from typing import Optional

from sarvalanche.utils.generators import iter_track_pol_combinations
from sarvalanche.weights.combinations import combine_weights
from sarvalanche.weights.polarizations import get_polarization_weights
from sarvalanche.utils.validation import validate_weights_sum_to_one
from sarvalanche.probabilities.combine import combine_probabilities
from sarvalanche.features.mahalanobis_distance import mahalanobis_distance
from sarvalanche.ml.inference import load_model
from sarvalanche.probabilities.z_score import z_score_to_probability

_DEFAULT_WEIGHTS = Path(__file__).parent.parent / 'ml' / 'weights' / 'sar_transformer_best.pth'

def calculate_ml_distances(
    ds: xr.Dataset,
    avalanche_date: np.datetime64,
    model_weights: Optional[Path] = _DEFAULT_WEIGHTS,
    validate_weights: bool = True,
    device: str = 'mps',
    stride: int = 4,
    batch_size: int = 128,
) -> xr.DataArray:
    """
    ML-based replacement for calculate_mahalanobis_distances.

    Uses the SAR transformer to predict the expected backscatter and its
    uncertainty, then computes a signed z-score distance per track/pol
    combination. Combines across tracks/pols using the same weighting and
    agreement-boosting logic as the Mahalanobis pipeline.

    Parameters
    ----------
    ds : xr.Dataset
        Dataset containing VV and VH backscatter with time and track coords.
    model : torch.nn.Module
        Loaded SARTransformer in eval mode.
    combination_method : str
        'weighted_mean' | 'max' | 'mean' | 'median'
    use_agreement_boosting : bool
        If True, boost signal when multiple tracks/pols agree on anomaly.
    agreement_strength : float
        Boosting magnitude (0-1). Only used if use_agreement_boosting=True.
    min_distance_threshold : float
        Z-score threshold for "detection" in agreement boosting.
    validate_weights : bool
        Validate that weights sum to 1.0.
    device : str
        Torch device ('mps', 'cuda', 'cpu').
    stride : int
        Inference stride for sweeping window.
    batch_size : int
        Inference batch size.

    Returns
    -------
    xr.DataArray
        Combined ML distance (signed z-score) across all track/pol combos.
    """
    results        = []
    resolution_weights = []
    pol_weights    = []
    track_pol_labels = []

    model = load_model(model_weights)

    model.eval()
    with torch.no_grad():
        for track, pol, da in iter_track_pol_combinations(ds):

            distance, sigma_da = mahalanobis_distance(da, avalanche_date, model, device=device,
                                             stride=stride, batch_size=batch_size)

            # Store per-combo products in dataset for reference / debugging
            ds[f'd_{track}_{pol}_ml']    = distance
            ds[f'sigma_{track}_{pol}_ml'] = sigma_da

            ds[f'd_{track}_{pol}_ml'].attrs = {
                'units':   'standard_deviations',
                'source':  'sarvalanche',
                'product': 'ml_transformer_distance',
            }

            results.append(distance)
            resolution_weights.append(ds['w_resolution'].sel(static_track=track))
            pol_weights.append(get_polarization_weights(pol))
            track_pol_labels.append(f"{track}_{pol}")

    if not results:
        raise ValueError('No results generated by ML distance calculation.')

    distances = xr.concat(results, dim='track_pol').assign_coords(track_pol=track_pol_labels)

    combined_distance = distances.where(
        lambda x: np.abs(x) == np.abs(distances).max(dim='track_pol'),
        drop=True
    ).max(dim='track_pol')

    probability = z_score_to_probability(combined_distance, threshold = 1.0)

    probability.attrs = {
        'source':             'sarvalanche',
        'units':              'probability [0, 1]',
        'product':            'combined_ml_probability',
        'method':             'transformer_z_score',
        'combination_method': 'max',
    }

    return probability